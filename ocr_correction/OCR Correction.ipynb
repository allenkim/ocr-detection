{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promotional-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ReformerModelWithLMHead, ReformerConfig, ReformerTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accredited-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_path = Path(\"/home/allekim/stonybook-data/hathi/ocr_model_results/double_books/\")\n",
    "result_paths = list(ocr_path.glob('*'))\n",
    "df = pd.read_csv(result_paths[1], converters={'ctx1': eval, 'ctx2': eval, 'diff1': eval, 'diff2': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fancy-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(row):\n",
    "    loss1, loss2 = row['loss1'], row['loss2']\n",
    "    diff1, diff2 = row['diff1'], row['diff2']\n",
    "    ctx1, ctx2 = row['ctx1'], row['ctx2']\n",
    "    if any(['*' in x for x in ctx1]) or any(['*' in x for x in ctx2]):\n",
    "        return np.nan\n",
    "    ocr1, ocr2 = ctx1[diff1[0]:diff1[1]], ctx2[diff2[0]:diff2[1]]\n",
    "    ex1 = ' '.join(ctx1[:diff1[0]]) + '*' + ' '.join(ocr1) + '*' + ' '.join(ctx1[diff1[1]:])\n",
    "    ex2 = ' '.join(ctx2[:diff2[0]]) + '*' + ' '.join(ocr2) + '*' + ' '.join(ctx2[diff2[1]:])\n",
    "    if loss1 < loss2:\n",
    "        correct = '#' + ' '.join(ocr1) + '#'    \n",
    "    else:\n",
    "        correct = '#' + ' '.join(ocr2) + '#'\n",
    "    return (ex1 + correct, ex2 + correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-cricket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consistent-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['examples'] = df.apply(generate_examples, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "short-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chicken-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [e for l in df['examples'].dropna() for e in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "clinical-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.encodings['input_ids'][idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ambient-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = OCRDataset(encode(result[:450]))\n",
    "test_data = OCRDataset(encode(result[450:]))\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "transparent-poverty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerModelWithLMHead(\n",
       "  (reformer): ReformerModel(\n",
       "    (embeddings): ReformerEmbeddings(\n",
       "      (word_embeddings): Embedding(258, 1024)\n",
       "      (position_embeddings): AxialPositionEmbeddings(\n",
       "        (weights): ParameterList(\n",
       "            (0): Parameter containing: [torch.FloatTensor of size 128x1x256]\n",
       "            (1): Parameter containing: [torch.FloatTensor of size 1x512x768]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): ReformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): ReformerOnlyLMHead(\n",
       "    (decoder): Linear(in_features=2048, out_features=258, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ReformerModelWithLMHead.from_pretrained(\"google/reformer-enwik8\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "underlying-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_warmup_steps = 500\n",
    "num_train_steps = 500\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "seventh-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:3\")\n",
    "# model = model.to(device)\n",
    "# x = next(iter(train_dataloader))\n",
    "# input_ids = x[\"input_ids\"].to(device)\n",
    "# attention_mask = x[\"attention_mask\"].to(device)\n",
    "# labels = x[\"labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "everyday-neutral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output = model(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aggregate-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "marked-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ex in train_dataloader:\n",
    "#     input_ids = ex[\"input_ids\"].to(device)\n",
    "#     attention_mask = ex[\"attention_mask\"].to(device)\n",
    "#     labels = ex[\"labels\"].to(device)\n",
    "#     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#     loss = outputs.loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     print(loss)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-virgin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
